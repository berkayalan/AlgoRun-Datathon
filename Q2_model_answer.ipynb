{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import metrics, model_selection\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets for Q1\n",
    "sales = pd.read_csv('algorun-2021-data/sales.csv')\n",
    "hierarc = pd.read_csv('algorun-2021-data/product_hierarchy.csv')\n",
    "store = pd.read_csv('algorun-2021-data/store_cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop promo_discount_type_2, promo_discount_2, promo_bin_2, promo_bin_1\n",
    "sales.isna().sum().sort_values(ascending=False)/len(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sales.drop([\"promo_discount_type_2\", \"promo_discount_2\", \"promo_bin_2\", \"promo_bin_1\"],axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales.dropna(subset=['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge product and sales dataframes\n",
    "df = pd.merge(sales, hierarc[['product_id', 'hierarchy1_id', 'hierarchy2_id', 'hierarchy3_id', 'hierarchy4_id']], left_on='product_id', right_on='product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending=False)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It will convert columns that have string dtypes to pandas category type\n",
    "def train_cats(df):\n",
    "    for n,c in df.items():\n",
    "        if is_string_dtype(c):\n",
    "            df[n] = c.astype(\"category\").cat.as_ordered()\n",
    "            \n",
    "df_categorized = df.copy()\n",
    "train_cats(df_categorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing of the entry can also be a valuable information\n",
    "# So we will create a column that is False when value is missing\n",
    "# We encoded missingness in categorical columns so we will just create _na columns for numerical types\n",
    "def fix_missing(df, col, name):\n",
    "    if is_numeric_dtype(col):\n",
    "        if pd.isnull(col).sum():\n",
    "            df[name+\"_na\"] = pd.isnull(col)\n",
    "        df[name] = col.fillna(col.median())\n",
    "        \n",
    "# We will have codes starting from 0 (for missing)\n",
    "def numericalize(df, col, name):\n",
    "    if not is_numeric_dtype(col):\n",
    "        df[name] = col.cat.codes+1\n",
    "          \n",
    "def proc_df(df):\n",
    "    \n",
    "#     y = df[y_fld].values\n",
    "#     df.drop([y_fld], axis = 1, inplace = True)\n",
    "    \n",
    "    for n, c in df.items():\n",
    "        fix_missing(df, c, n)\n",
    "        \n",
    "    for n, c in df.items():\n",
    "        numericalize(df, c, n)\n",
    "    \n",
    "#     y = df[y_fld].values\n",
    "#     df.drop([y_fld], axis = 1, inplace = True)\n",
    "    \n",
    "#     res = [df, y]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visual = df_categorized.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visual = proc_df(df_visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En yüksek korelasyon date ve price ile arasında var\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(df_visual.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt_name is the name of the column that is of type datepart\n",
    "def add_datepart(df, dt_name, drop=True, time=False):\n",
    "    \"Creates new columns from our datetime column\"\n",
    "    \n",
    "    \n",
    "    dt_column = df[dt_name]\n",
    "    column_dtype = dt_column.dtype\n",
    "    \n",
    "\n",
    "    targ_name = re.sub('[Dd]ate$', '', dt_name)\n",
    "    \n",
    "    # attributes are normally in lower case but we wrote this way because we will use it in columns' name too\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    \n",
    "    if time: \n",
    "        attr = attr + ['Hour', 'Minute', 'Second']\n",
    "        \n",
    "    \n",
    "    #Sorry curse of dimensionality, maybe another time\n",
    "    for a in attr: \n",
    "        df[targ_name + a] = getattr(dt_column.dt, a.lower())\n",
    "        \n",
    "    # how much time passed, we will divide by 10^9 because it is in the nanosecond format\n",
    "    df[targ_name + 'Elapsed'] = dt_column.astype(np.int64) // 10 ** 9\n",
    "    \n",
    "    if drop: \n",
    "        df.drop(dt_name, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.copy()\n",
    "df_new.date = pd.to_datetime(df_new.date)\n",
    "add_datepart(df_new, 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = pd.DataFrame(df.groupby(by=[\"product_id\",\"hierarchy4_id\",\"date\"],as_index=False)[\"sales\"].sum())\n",
    "df_sales.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_2017 = df_sales[(df_sales[\"date\"]>=\"2017-01-01\") & (df_sales[\"date\"]<=\"2017-12-31\")]\n",
    "df_sales_2018 = df_sales[(df_sales[\"date\"]>=\"2018-01-01\") & (df_sales[\"date\"]<=\"2018-12-31\")]\n",
    "df_sales_2017.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_2017.groupby(by=\"hierarchy4_id\").nunique().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2018 yılı için yıl boyunca aynı hiyerarşi 4 id'sine sahip ürünler benzer satış yükseliş ve artışı göstermiş.\n",
    "# hierarchy4_id=H00000405 \n",
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(df_sales_2017[df_sales_2017[\"product_id\"]=='P0110'][\"date\"], df_sales_2017[df_sales_2017[\"product_id\"]=='P0110'][\"sales\"], \"b\")\n",
    "plt.plot(df_sales_2017[df_sales_2017[\"product_id\"]=='P0249'][\"date\"], df_sales_2017[df_sales_2017[\"product_id\"]=='P0249'][\"sales\"], \"r\")\n",
    "plt.plot(df_sales_2017[df_sales_2017[\"product_id\"]=='P0265'][\"date\"], df_sales_2017[df_sales_2017[\"product_id\"]=='P0265'][\"sales\"], \"m\")\n",
    "plt.plot(df_sales_2017[df_sales_2017[\"product_id\"]=='P0427'][\"date\"], df_sales_2017[df_sales_2017[\"product_id\"]=='P0427'][\"sales\"], \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2018 yılı için yıl boyunca aynı hiyerarşi 4 id'sine sahip ürünler benzer satış yükseliş ve artışı göstermiş.\n",
    "# hierarchy4_id=H00000405 \n",
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(df_sales_2018[df_sales_2018[\"product_id\"]=='P0116'][\"date\"], df_sales_2018[df_sales_2018[\"product_id\"]=='P0116'][\"sales\"], \"b\")\n",
    "plt.plot(df_sales_2018[df_sales_2018[\"product_id\"]=='P0198'][\"date\"], df_sales_2018[df_sales_2018[\"product_id\"]=='P0198'][\"sales\"], \"r\")\n",
    "plt.plot(df_sales_2018[df_sales_2018[\"product_id\"]=='P0590'][\"date\"], df_sales_2018[df_sales_2018[\"product_id\"]=='P0590'][\"sales\"], \"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample\n",
    "def get_sample(df,n):\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    idxs = np.random.permutation(len(df))[:n]\n",
    "    return idxs, df.iloc[idxs].copy()\n",
    "\n",
    "# Creating validation set\n",
    "# It will split our data set to have length n train and len(df) - n validation set\n",
    "def split_train_val(df,n): \n",
    "    return df[:n].copy(), df[n:].copy()\n",
    "\n",
    "# Creating validation set\n",
    "# It will split our data set to have length n train and len(df) - n validation set\n",
    "def split_train_val(df,n): \n",
    "    return df[:n].copy(), df[n:].copy()\n",
    "\n",
    "def rmse(x,y): \n",
    "    return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m):  \n",
    "    print(f\"RMSE of train set {rmse(m.predict(X_train), y_train)}\")\n",
    "    print(f\"RMSE of validation set {rmse(m.predict(X_valid), y_valid)}\")\n",
    "    print(f\"R^2 of train set {m.score(X_train, y_train)}\")\n",
    "    print(f\"R^2 of validation set {m.score(X_valid, y_valid)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cats(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop([\"product_id\"], axis = 1)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing of the entry can also be a valuable information\n",
    "# So we will create a column that is False when value is missing\n",
    "# We encoded missingness in categorical columns so we will just create _na columns for numerical types\n",
    "def fix_missing(df, col, name):\n",
    "    if is_numeric_dtype(col):\n",
    "        if pd.isnull(col).sum():\n",
    "            df[name+\"_na\"] = pd.isnull(col)\n",
    "        df[name] = col.fillna(col.median())\n",
    "        \n",
    "# We will have codes starting from 0 (for missing)\n",
    "def numericalize(df, col, name):\n",
    "    if not is_numeric_dtype(col):\n",
    "        df[name] = col.cat.codes+1\n",
    "          \n",
    "def proc_df(df, y_fld):\n",
    "    \n",
    "#     y = df[y_fld].values\n",
    "#     df.drop([y_fld], axis = 1, inplace = True)\n",
    "    \n",
    "    for n, c in df.items():\n",
    "        fix_missing(df, c, n)\n",
    "        \n",
    "    for n, c in df.items():\n",
    "        numericalize(df, c, n)\n",
    "    \n",
    "    y = df[y_fld].values\n",
    "    df.drop([y_fld], axis = 1, inplace = True)\n",
    "    \n",
    "    res = [df, y]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hier1, y_hier1 = proc_df(df_new.drop(columns=['hierarchy3_id', 'hierarchy1_id', 'hierarchy2_id']), 'hierarchy4_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hier1.shape, y_hier1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hier1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample, nc, y_sample,na = model_selection.train_test_split(df_hier1, y_hier1, train_size = 0.005, random_state = 42, stratify=y_hier1)\n",
    "X_sample.shape, y_sample.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(X_sample, y_sample, train_size = 0.8, random_state = 42, stratify=y_sample)\n",
    "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_test, y_train_2, y_test = model_selection.train_test_split(X_train, y_train, train_size = 0.9, random_state = 42, stratify=y_train)\n",
    "X_train_2.shape, X_test.shape, y_train_2.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=100, max_depth=3, bootstrap=True, n_jobs=-1)\n",
    "%time m.fit(X_train_2, y_train_2)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = XGBClassifier(learning_rate=0.4, n_estimators=220, max_depth=5,\n",
    "                        min_child_weight=3, subsample=0.6,\n",
    "                        objective='binary:logistic', nthread=4, seed=27)\n",
    "%time alg.fit(X_train_2, y_train_2)\n",
    "print_score(alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_feat_importance(alg, df):\n",
    "    return pd.DataFrame({'columns':df.columns, 'importance':alg.feature_importances_}\n",
    "                       ).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_feat_importance(alg, X_train_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
